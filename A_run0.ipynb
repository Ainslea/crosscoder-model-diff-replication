{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EnNEoS_CV__e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "EnNEoS_CV__e",
        "outputId": "41cfcee8-85ac-4faf-c287-dd9cb6421978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-2.16.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (1.10.1)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.8.1)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting numpy<2,>=1.26 (from transformer_lens)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.2.1)\n",
            "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.51 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.56.1)\n",
            "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer_lens)\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.15.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.9)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (0.22.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.11.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.37.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (3.12.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.6->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.6->transformer_lens) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Downloading transformer_lens-2.16.1-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.0/192.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: transformers-stream-generator\n",
            "  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12426 sha256=34080db500a816d4c8e7f072e565e3a3201e3175338fcef7a82506aed1f573d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/58/d2/014cb67c3cc6def738c1b1635dbf4e3dab6fb63aba7070dce0\n",
            "Successfully built transformers-stream-generator\n",
            "Installing collected packages: better-abc, wadler-lindig, numpy, fancy-einsum, beartype, jaxtyping, transformers-stream-generator, transformer_lens\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: beartype\n",
            "    Found existing installation: beartype 0.21.0\n",
            "    Uninstalling beartype-0.21.0:\n",
            "      Successfully uninstalled beartype-0.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plum-dispatch 2.5.7 requires beartype>=0.16.2, but you have beartype 0.14.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beartype-0.14.1 better-abc-0.0.3 fancy-einsum-0.0.3 jaxtyping-0.3.2 numpy-1.26.4 transformer_lens-2.16.1 transformers-stream-generator-0.0.5 wadler-lindig-0.1.7\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "831e7d2c81cc496d8bcdd22d7d88c57f",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install transformer_lens huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NibB4q67Pbku",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NibB4q67Pbku",
        "outputId": "0e2b034d-4aa5-4b69-9a53-6e7e225dd03e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o5nKaksrR_mE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5nKaksrR_mE",
        "outputId": "067bf702-e0fd-4ae9-fc5a-9401752069c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bwnnwfLAWuu7",
      "metadata": {
        "id": "bwnnwfLAWuu7"
      },
      "outputs": [],
      "source": [
        "# notebook_login()\n",
        "from huggingface_hub import login\n",
        "login(token='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b55cf78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b55cf78",
        "outputId": "61a8c143-1613-4f87-cdd3-e1eba42ff826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-20 23:14:23.463969: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-09-20 23:14:23.482227: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758410063.503555    3825 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758410063.510124    3825 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758410063.526709    3825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758410063.526739    3825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758410063.526742    3825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758410063.526745    3825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-20 23:14:23.531686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n",
            "config.json: 100% 818/818 [00:00<00:00, 5.59MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors.index.json: 100% 24.2k/24.2k [00:00<00:00, 78.6MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/481M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 11.8M/481M [00:01<00:59, 7.87MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  16% 78.9M/481M [00:01<00:06, 58.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 280M/481M [00:01<00:00, 239MB/s]  \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors: 100% 481M/481M [00:02<00:00, 229MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 689k/4.98G [00:02<4:30:57, 306kB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 67.8M/4.98G [00:02<02:20, 35.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 727k/4.99G [00:02<5:15:16, 264kB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   1% 67.8M/4.99G [00:02<02:29, 32.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   4% 202M/4.99G [00:02<00:40, 118MB/s]  \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   7% 336M/4.99G [00:03<00:21, 217MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 135M/4.98G [00:03<01:19, 60.7MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   9% 470M/4.99G [00:03<00:15, 301MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 337M/4.98G [00:03<00:30, 151MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  12% 604M/4.99G [00:03<00:13, 322MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 404M/4.98G [00:04<00:28, 163MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 472M/4.98G [00:04<00:28, 156MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  13% 671M/4.99G [00:04<00:21, 201MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 539M/4.98G [00:05<00:30, 147MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 740M/4.99G [00:05<00:23, 179MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 606M/4.98G [00:05<00:24, 180MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  16% 807M/4.99G [00:06<00:33, 126MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 673M/4.98G [00:06<00:46, 91.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  18% 874M/4.99G [00:07<00:43, 94.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 740M/4.98G [00:07<00:43, 98.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  19% 941M/4.99G [00:07<00:35, 114MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 809M/4.98G [00:07<00:38, 110MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:08<00:42, 94.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 876M/4.98G [00:08<00:46, 89.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  23% 1.14G/4.99G [00:09<00:29, 131MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  24% 1.21G/4.99G [00:11<00:54, 69.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 943M/4.98G [00:11<01:17, 52.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:11<00:32, 112MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  28% 1.41G/4.99G [00:11<00:26, 137MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.98G [00:11<00:35, 107MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  30% 1.48G/4.99G [00:11<00:21, 164MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/4.98G [00:12<00:30, 120MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  31% 1.54G/4.99G [00:12<00:28, 121MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.35G/4.98G [00:15<00:54, 66.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  32% 1.61G/4.99G [00:15<01:01, 54.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.98G [00:16<00:51, 69.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  34% 1.68G/4.99G [00:16<00:56, 59.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.48G/4.98G [00:17<00:56, 62.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  35% 1.75G/4.99G [00:17<00:55, 58.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.55G/4.98G [00:18<00:47, 71.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  36% 1.81G/4.99G [00:18<00:48, 65.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.61G/4.98G [00:18<00:42, 79.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  38% 1.88G/4.99G [00:19<00:42, 72.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.98G [00:19<00:42, 78.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  39% 1.95G/4.99G [00:19<00:35, 86.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.75G/4.98G [00:20<00:45, 70.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  40% 2.01G/4.99G [00:20<00:39, 75.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  42% 2.08G/4.99G [00:21<00:34, 84.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.81G/4.98G [00:22<00:49, 64.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  43% 2.15G/4.99G [00:22<00:34, 83.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.98G [00:22<00:41, 73.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  44% 2.21G/4.99G [00:22<00:29, 94.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.98G [00:23<00:34, 88.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.98G [00:25<00:44, 65.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  46% 2.28G/4.99G [00:25<00:57, 47.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.98G [00:26<00:22, 118MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  47% 2.35G/4.99G [00:26<00:47, 55.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.98G [00:26<00:17, 144MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.48G/4.98G [00:27<00:21, 115MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  48% 2.42G/4.99G [00:27<00:46, 55.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.98G [00:28<00:16, 147MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.69G/4.98G [00:29<00:18, 125MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  50% 2.48G/4.99G [00:29<00:47, 53.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/4.98G [00:29<00:20, 111MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  51% 2.55G/4.99G [00:29<00:40, 61.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.98G [00:30<00:15, 138MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 2.97G/4.98G [00:31<00:14, 135MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  52% 2.62G/4.99G [00:31<00:46, 50.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.04G/4.98G [00:31<00:15, 123MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.11G/4.98G [00:32<00:16, 113MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  55% 2.75G/4.99G [00:33<00:35, 63.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.17G/4.98G [00:33<00:17, 106MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  57% 2.82G/4.99G [00:34<00:31, 68.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.24G/4.98G [00:34<00:17, 102MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.30G/4.98G [00:34<00:13, 123MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  58% 2.89G/4.99G [00:34<00:27, 75.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.37G/4.98G [00:35<00:14, 110MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.44G/4.98G [00:35<00:16, 95.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  62% 3.09G/4.99G [00:35<00:18, 102MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.50G/4.98G [00:36<00:15, 93.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  63% 3.16G/4.99G [00:36<00:18, 98.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.57G/4.98G [00:37<00:15, 90.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  65% 3.22G/4.99G [00:37<00:18, 94.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.64G/4.98G [00:38<00:14, 90.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  67% 3.36G/4.99G [00:38<00:16, 99.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.71G/4.98G [00:38<00:13, 97.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  69% 3.42G/4.99G [00:39<00:14, 105MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.77G/4.98G [00:39<00:12, 96.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.84G/4.98G [00:40<00:11, 101MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  70% 3.49G/4.99G [00:40<00:16, 89.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.91G/4.98G [00:40<00:09, 113MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.97G/4.98G [00:41<00:08, 115MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  71% 3.56G/4.99G [00:41<00:15, 89.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.04G/4.98G [00:41<00:09, 103MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  73% 3.62G/4.99G [00:42<00:15, 87.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  74% 3.69G/4.99G [00:43<00:17, 74.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.11G/4.98G [00:43<00:12, 69.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  75% 3.76G/4.99G [00:44<00:20, 59.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.18G/4.98G [00:45<00:13, 61.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  77% 3.83G/4.99G [00:45<00:14, 79.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.25G/4.98G [00:46<00:12, 58.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  78% 3.89G/4.99G [00:46<00:16, 68.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.31G/4.98G [00:48<00:15, 44.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  79% 3.96G/4.99G [00:50<00:30, 33.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.38G/4.98G [00:50<00:15, 39.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  82% 4.09G/4.99G [00:50<00:14, 60.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.51G/4.98G [00:50<00:06, 70.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  84% 4.17G/4.99G [00:51<00:10, 80.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.65G/4.98G [00:51<00:02, 112MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  86% 4.31G/4.99G [00:51<00:05, 128MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.78G/4.98G [00:51<00:01, 147MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  89% 4.44G/4.99G [00:52<00:04, 128MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.85G/4.98G [00:52<00:01, 115MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  91% 4.52G/4.99G [00:52<00:03, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  92% 4.59G/4.99G [00:53<00:02, 151MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.92G/4.98G [00:53<00:00, 114MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  93% 4.66G/4.99G [00:53<00:01, 173MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 4.98G/4.98G [00:53<00:00, 93.3MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  97% 4.86G/4.99G [00:53<00:00, 285MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [00:53<00:00, 92.6MB/s]\n",
            "Fetching 3 files: 100% 3/3 [00:54<00:00, 18.16s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:00<00:00, 37.10it/s]\n",
            "generation_config.json: 100% 168/168 [00:00<00:00, 1.45MB/s]\n",
            "tokenizer_config.json: 100% 46.4k/46.4k [00:00<00:00, 124MB/s]\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 6.32MB/s]\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 26.8MB/s]\n",
            "special_tokens_map.json: 100% 636/636 [00:00<00:00, 5.25MB/s]\n",
            "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
            "Loaded pretrained model gemma-2-2b into HookedTransformer\n",
            "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n",
            "config.json: 100% 838/838 [00:00<00:00, 6.88MB/s]\n",
            "model.safetensors.index.json: 100% 24.2k/24.2k [00:00<00:00, 78.0MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/241M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 900k/4.99G [00:01<1:39:12, 838kB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 67.9M/4.99G [00:01<01:29, 55.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 39.4M/241M [00:01<00:08, 23.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 107M/241M [00:01<00:02, 64.5MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 472M/4.99G [00:02<00:14, 321MB/s]  \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 241M/241M [00:02<00:00, 107MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.99G [00:02<00:06, 626MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.99G [00:04<00:16, 232MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.99G [00:05<00:18, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.99G [00:05<00:19, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.99G [00:06<00:18, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.99G [00:06<00:17, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.99G [00:06<00:14, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.99G [00:06<00:12, 268MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/4.99G [00:06<00:10, 312MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.99G [00:07<00:09, 346MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.99G [00:07<00:10, 289MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.99G [00:07<00:05, 505MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.99G [00:07<00:06, 452MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 2.22G/4.99G [00:08<00:06, 417MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.99G [00:08<00:13, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.99G [00:09<00:13, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 2.42G/4.99G [00:09<00:11, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.99G [00:09<00:10, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 2.55G/4.99G [00:10<00:11, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.99G [00:10<00:12, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.99G [00:11<00:12, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 2.76G/4.99G [00:14<00:38, 58.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.99G [00:14<00:30, 72.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/4.99G [00:14<00:21, 96.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.99G [00:14<00:11, 167MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/4.99G [00:14<00:07, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/4.99G [00:14<00:04, 354MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.99G [00:15<00:03, 466MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 3.56G/4.99G [00:15<00:02, 582MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.99G [00:16<00:05, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.99G [00:16<00:04, 281MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.99G [00:16<00:03, 295MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 3.96G/4.99G [00:17<00:03, 311MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.99G [00:17<00:03, 268MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.99G [00:17<00:03, 288MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 4.16G/4.99G [00:19<00:07, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 4.26G/4.99G [00:20<00:07, 102MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.99G [00:20<00:03, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.99G [00:20<00:01, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 4.66G/4.99G [00:20<00:01, 287MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.99G [00:21<00:00, 305MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/4.99G [00:21<00:00, 320MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 4.86G/4.99G [00:21<00:00, 337MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.99G [00:21<00:00, 345MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 4.99G/4.99G [00:21<00:00, 229MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:22<00:00, 11.13s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.50it/s]\n",
            "generation_config.json: 100% 187/187 [00:00<00:00, 1.59MB/s]\n",
            "tokenizer_config.json: 100% 47.0k/47.0k [00:00<00:00, 144MB/s]\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 5.63MB/s]\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 21.8MB/s]\n",
            "special_tokens_map.json: 100% 636/636 [00:00<00:00, 5.26MB/s]\n",
            "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
            "Loaded pretrained model gemma-2-2b-it into HookedTransformer\n",
            "Loading data from disk\n",
            "Data is not cached. Loading data from HF\n",
            "README.md: 100% 292/292 [00:00<00:00, 2.50MB/s]\n",
            "data/train-00000-of-00008.parquet: 100% 209M/209M [00:09<00:00, 22.3MB/s]\n",
            "data/train-00001-of-00008.parquet: 100% 209M/209M [00:07<00:00, 28.0MB/s]\n",
            "data/train-00002-of-00008.parquet: 100% 209M/209M [00:07<00:00, 28.8MB/s]\n",
            "data/train-00003-of-00008.parquet: 100% 209M/209M [00:07<00:00, 29.3MB/s] \n",
            "data/train-00004-of-00008.parquet: 100% 210M/210M [00:07<00:00, 28.8MB/s]\n",
            "data/train-00005-of-00008.parquet: 100% 209M/209M [00:07<00:00, 28.9MB/s]\n",
            "data/train-00006-of-00008.parquet: 100% 209M/209M [00:07<00:00, 29.7MB/s]\n",
            "data/train-00007-of-00008.parquet: 100% 209M/209M [00:07<00:00, 29.7MB/s]\n",
            "Generating train split: 100% 963566/963566 [00:14<00:00, 68337.58 examples/s]\n",
            "Saving the dataset (8/8 shards): 100% 963566/963566 [00:06<00:00, 149051.48 examples/s]\n",
            "Saved tokens to disk\n",
            "Updated config\n",
            "{\n",
            "  \"seed\": 49,\n",
            "  \"batch_size\": 4096,\n",
            "  \"buffer_mult\": 128,\n",
            "  \"lr\": 5e-05,\n",
            "  \"num_tokens\": 400000000,\n",
            "  \"l1_coeff\": 2,\n",
            "  \"beta1\": 0.9,\n",
            "  \"beta2\": 0.999,\n",
            "  \"d_in\": 2304,\n",
            "  \"dict_size\": 16384,\n",
            "  \"seq_len\": 1024,\n",
            "  \"enc_dtype\": \"fp32\",\n",
            "  \"model_name\": \"gemma-2-2b\",\n",
            "  \"site\": \"resid_pre\",\n",
            "  \"device\": \"cuda:0\",\n",
            "  \"model_batch_size\": 4,\n",
            "  \"log_every\": 100,\n",
            "  \"save_every\": 30000,\n",
            "  \"dec_init_norm\": 0.08,\n",
            "  \"hook_point\": \"blocks.14.hook_resid_pre\",\n",
            "  \"wandb_project\": \"YOUR_WANDB_PROJECT\",\n",
            "  \"wandb_entity\": \"YOUR_WANDB_ENTITY\"\n",
            "}\n",
            "Estimating norm scaling factor: 100% 100/100 [01:46<00:00,  1.06s/it]\n",
            "Estimating norm scaling factor: 100% 100/100 [01:45<00:00,  1.06s/it]\n",
            "Refreshing the buffer!\n",
            "100% 128/128 [00:55<00:00,  2.29it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory. Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/wandb/offline-run-20250920_232614-ef2ynk4a\u001b[0m\n",
            "  0% 0/97656 [00:00<?, ?it/s]{'loss': 4856.16943359375, 'l2_loss': 4856.16943359375, 'l1_loss': 117.15872955322266, 'l0_loss': 8172.73779296875, 'l1_coeff': 0.0, 'lr': 5e-05, 'explained_variance': -0.5003828406333923, 'explained_variance_A': -0.5173124670982361, 'explained_variance_B': -0.4852457344532013}\n",
            "  0% 62/97656 [00:11<4:54:26,  5.52it/s]Refreshing the buffer!\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\n",
            "  0% 62/97656 [00:11<5:02:59,  5.37it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/trainer.py\", line 76, in train\n",
            "    loss_dict = self.step()\n",
            "                ^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/trainer.py\", line 42, in step\n",
            "    acts = self.buffer.next()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/buffer.py\", line 112, in next\n",
            "    self.refresh()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/buffer.py\", line 79, in refresh\n",
            "    _, cache_A = self.model_A.run_with_cache(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformer_lens/HookedTransformer.py\", line 702, in run_with_cache\n",
            "    out, cache_dict = super().run_with_cache(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformer_lens/hook_points.py\", line 560, in run_with_cache\n",
            "    model_out = self(*model_args, **model_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformer_lens/HookedTransformer.py\", line 591, in forward\n",
            "    ) = self.input_to_embed(\n",
            "        ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformer_lens/HookedTransformer.py\", line 417, in input_to_embed\n",
            "    embed = self.hook_embed(self.embed(tokens))  # [batch, pos, d_model]\n",
            "                            ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformer_lens/components/embed.py\", line 34, in forward\n",
            "    return self.W_E[tokens, :]\n",
            "           ~~~~~~~~^^^^^^^^^^^\n",
            "IndexError: tensors used as indices must be long, int, byte or bool tensors\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/train.py\", line 48, in <module>\n",
            "    trainer.train()\n",
            "  File \"/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/trainer.py\", line 82, in train\n",
            "    self.save()\n",
            "  File \"/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/trainer.py\", line 70, in save\n",
            "    self.crosscoder.save()\n",
            "  File \"/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/crosscoder.py\", line 143, in save\n",
            "    self.create_save_dir()\n",
            "  File \"/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/crosscoder.py\", line 131, in create_save_dir\n",
            "    for file in list(SAVE_DIR.iterdir())\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/pathlib.py\", line 1056, in iterdir\n",
            "    for name in os.listdir(self):\n",
            "                ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/workspace/crosscoder-model-diff-replication/checkpoints'\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/wandb/offline-run-20250920_232614-ef2ynk4a\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20250920_232614-ef2ynk4a/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#!python '/content/drive/MyDrive/kxcoder0/crosscoder-model-diff-replication/train.py'\n",
        "!python 'train.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CQBgUugHespK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "CQBgUugHespK",
        "outputId": "9a8d2298-2973-49df-9023-084cdb4ecc5e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-921164186.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
